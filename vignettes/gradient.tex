\title{Autoencoder Gradient}
\author{
       Felix Brechtmann
}
\date{\today}

\documentclass[11pt]{letter}

\usepackage{amsmath}

%\newcommand{\matr}[1]{#1}
\newcommand{\matr}[1]{\mathbf{#1}}

\begin{document}

\textbf{Supplemental Methods}

\textbf{Negative Binomial model}

In this paper we use the following parameterization for the Negative binomial distribution.

\begin{align*}
P(k| \mu, \theta) = \frac{\Gamma(k + \theta)}{\Gamma(k) \theta!}  
\left ( \frac{\mu}{\mu + \theta} \right )^{k}
\left ( \frac{\theta}{\mu + \theta} \right)^{\theta} 
\end{align*}

where the variance of the distribution is given by:
\[
Var = \mu + \frac{\mu^2}{\theta}
\]
and hence the coefficient of variation is given by:
\[
CV^2 = \frac{1}{\mu} + \frac{1}{\theta}
\]


\textbf{Computation of the default theta}

We assume, a biological covariation of 20\% for large means.

\begin{align*}
CV^2 =& \frac{1}{\mu} + \frac{1}{\theta}\\
\lim_{\mu \to \infty} CV^2 =& \frac{1}{\theta}\\
\theta \approx& \frac{1}{CV^2}
\end{align*}
and hence equate a default $\theta = 25$.





\textbf{Autoencoder Gradient}

We use L-BFGS to fit the autoencoder model as described in the Methods.
To speed up the fitting we implemented the gradient as derived below.

The expectations $\mu$ are modeled by:
\begin{align*}
\mu_{ij} &= s_{i} e^{y_{ij} + \bar{x}_j}\\
\matr{Y} &= \matr{X} \matr{W} \matr{W}^T + \matr{b}
\end{align*}
where the matrix $\matr{X}$ is given by the matrix: $\log{\frac{k_{ij}+1}{s_i}} - \bar{x}_j$. 

The negative binomial log likelihood is given by:
\begin{align*}
ll=& \sum_{ij} k_{ij} \log{(\mu_{ij})} + 
\sum_{ij} \theta \log{(\theta)} -
\sum_{ij} (k_{ij} + \theta) \log{(\mu_{ij} + \theta)} \\
+&\sum_{ij} \log{(\Gamma(\theta + k_{ij}))} 
- \sum_{ij} \log{(\Gamma({\theta}) k_{ij}!)}
\end{align*}

For the derivation of the gradient only the first and third term need to be considered, 
as all other terms are independent of $\mu$.

Computing the derivative of the first term with respect to the matrix $\matr{W}$ by subsituting the autoencoder model for $\mu$. 
Here the operations $\log[\matr{A}]$ and $\exp[\matr{A}]$ are understood to be element-wise for a matrix or vector $\matr{A}$,

\begin{align*}
&\frac{d}{dw_{ab}}\sum_{ij} k_{ij} \log{(\mu_{ij})} \\
&= \frac{d}{dw_{ab}}\sum_{ij} k_{ij} \log{[\exp{[\matr{X} \matr{W} \matr{W}^T + \matr{b}]}]} \\
&= \frac{d}{dw_{ab}}\sum_{ij} k_{ij} \left (\matr{X} \matr{W} \matr{W}^T + \matr{b} \right ) \\
&= \frac{d}{dw_{ab}}\sum_{ij} k_{ij} \left (\sum_{lm} x_{il} w_{lm} w_{jm} + b_j \right ) \\
&= \sum_{ij} k_{ij} \left (x_{ia} w_{jb} + \delta_{aj} \sum_{l} x_{il}w_{lb} \right ) \\
&= \sum_{ij} x_{ia} k_{ij} w_{jb} + \sum_{il} k_{ia} x_{il} w_{lb}. \\
\end{align*}
Which can be written as:
\begin{align*}
\matr{K}^T \matr{X} \matr{W} - \matr{X}^T \matr{K} \matr{W}
\end{align*}

Equivalently the derivative of the third term is:
\begin{align*}
-\matr{L}^T \matr{X} \matr{W} - \matr{X}^T \matr{L} \matr{W}
\end{align*}
where the components of the matrix $\matr{L}$ are computed by:
\begin{align*}
l_{ij} = \frac{(k_{ij} + \theta) \mu_{ij}}{\theta + \mu_{ij}}   
\end{align*}

The combined result is then:
\begin{align*}
\frac{dll}{d\matr{W}} = \matr{K}^T \matr{X} \matr{W} + \matr{X}^T \matr{K} \matr{W} - 
\matr{L}^T \matr{X} \matr{W} - \matr{X}^T \matr{L} \matr{W}
\end{align*}


The derivative of the first term with respect to the bias $\matr{b}$ is computed as:

\begin{align*}
&\frac{d}{db_{a}}\sum_{ij} k_{ij} \log{(\mu_{ij})} \\
&= \frac{d}{db_{a}}\sum_{ij} k_{ij} \left (\sum_{lm} x_{il} w_{lm} w_{jm} + b_j \right) \\
&= \sum_{i} k_{ia}\\
\end{align*}

Equivalently for the third therm the derivative is $-\sum_{i} l_{ia}$ and so the derivative of the loglikelihood with respect to the bias is:

\begin{align*}
\frac{dll}{db_a} = \sum_{i} k_{ia} - l_{ia}\\
\end{align*}



\end{document}
